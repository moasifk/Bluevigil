2018-04-28 20:03:40,305 - org.apache.zookeeper.ClientCnxn - INFO - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1630a212258004a, negotiated timeout = 40000
2018-04-28 20:03:40,386 - org.apache.zookeeper.ZooKeeper - INFO - Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=90000 watcher=hconnection-0x12e7c800x0, quorum=127.0.0.1:2181, baseZNode=/hbase
2018-04-28 20:03:40,389 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-28 20:03:40,389 - org.apache.hadoop.hbase.mapreduce.TableOutputFormat - INFO - Created table instance for http_hb_tbl
2018-04-28 20:03:40,390 - org.apache.zookeeper.ClientCnxn - INFO - Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-04-28 20:03:40,390 - org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation - INFO - Closing zookeeper sessionid=0x1630a212258004a
2018-04-28 20:03:40,420 - org.apache.zookeeper.ZooKeeper - INFO - Session: 0x1630a212258004a closed
2018-04-28 20:03:40,422 - org.apache.spark.executor.Executor - INFO - Finished task 0.0 in stage 9.0 (TID 12). 1901 bytes result sent to driver
2018-04-28 20:03:40,421 - org.apache.zookeeper.ClientCnxn - INFO - EventThread shut down
2018-04-28 20:03:40,427 - org.apache.spark.scheduler.TaskSetManager - INFO - Finished task 0.0 in stage 9.0 (TID 12) in 337 ms on localhost (1/2)
2018-04-28 20:03:40,476 - org.apache.zookeeper.ClientCnxn - INFO - Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x1630a212258004b, negotiated timeout = 40000
2018-04-28 20:03:40,480 - org.apache.hadoop.hbase.mapreduce.TableOutputFormat - INFO - Created table instance for http_hb_tbl
2018-04-28 20:03:42,384 - org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation - INFO - Closing zookeeper sessionid=0x1630a212258004b
2018-04-28 20:03:42,415 - org.apache.zookeeper.ZooKeeper - INFO - Session: 0x1630a212258004b closed
2018-04-28 20:03:42,416 - org.apache.zookeeper.ClientCnxn - INFO - EventThread shut down
2018-04-28 20:03:42,517 - org.apache.spark.executor.Executor - INFO - Finished task 1.0 in stage 9.0 (TID 13). 1901 bytes result sent to driver
2018-04-28 20:03:42,520 - org.apache.spark.scheduler.DAGScheduler - INFO - ResultStage 9 (saveAsNewAPIHadoopDataset at BluevigilConsumer.java:155) finished in 2.431 s
2018-04-28 20:03:42,520 - org.apache.spark.scheduler.TaskSetManager - INFO - Finished task 1.0 in stage 9.0 (TID 13) in 2429 ms on localhost (2/2)
2018-04-28 20:03:42,521 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Removed TaskSet 9.0, whose tasks have all completed, from pool 
2018-04-28 20:03:42,521 - org.apache.spark.scheduler.DAGScheduler - INFO - Job 9 finished: saveAsNewAPIHadoopDataset at BluevigilConsumer.java:155, took 2.461399 s
2018-04-28 20:03:42,522 - org.apache.spark.streaming.scheduler.JobScheduler - INFO - Finished job streaming job 1524935020000 ms.0 from job set of time 1524935020000 ms
2018-04-28 20:03:42,522 - org.apache.spark.streaming.scheduler.JobScheduler - INFO - Starting job streaming job 1524935020000 ms.1 from job set of time 1524935020000 ms
2018-04-28 20:03:42,529 - org.apache.spark.SparkContext - INFO - Starting job: print at BluevigilConsumer.java:200
2018-04-28 20:03:42,532 - org.apache.spark.scheduler.DAGScheduler - INFO - Got job 10 (print at BluevigilConsumer.java:200) with 1 output partitions
2018-04-28 20:03:42,532 - org.apache.spark.scheduler.DAGScheduler - INFO - Final stage: ResultStage 10 (print at BluevigilConsumer.java:200)
2018-04-28 20:03:42,532 - org.apache.spark.scheduler.DAGScheduler - INFO - Parents of final stage: List()
2018-04-28 20:03:42,532 - org.apache.spark.scheduler.DAGScheduler - INFO - Missing parents: List()
2018-04-28 20:03:42,533 - org.apache.spark.scheduler.DAGScheduler - INFO - Submitting ResultStage 10 (MapPartitionsRDD[11] at mapToPair at BluevigilConsumer.java:136), which has no missing parents
2018-04-28 20:03:42,535 - org.apache.spark.storage.MemoryStore - INFO - Block broadcast_10 stored as values in memory (estimated size 4.6 KB, free 116.9 KB)
2018-04-28 20:03:42,539 - org.apache.spark.storage.MemoryStore - INFO - Block broadcast_10_piece0 stored as bytes in memory (estimated size 2.6 KB, free 119.5 KB)
2018-04-28 20:03:42,540 - org.apache.spark.storage.BlockManagerInfo - INFO - Added broadcast_10_piece0 in memory on localhost:42133 (size: 2.6 KB, free: 257.8 MB)
2018-04-28 20:03:42,541 - org.apache.spark.SparkContext - INFO - Created broadcast 10 from broadcast at DAGScheduler.scala:1006
2018-04-28 20:03:42,541 - org.apache.spark.scheduler.DAGScheduler - INFO - Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[11] at mapToPair at BluevigilConsumer.java:136)
2018-04-28 20:03:42,541 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Adding task set 10.0 with 1 tasks
2018-04-28 20:03:42,551 - org.apache.spark.scheduler.TaskSetManager - INFO - Starting task 0.0 in stage 10.0 (TID 14, localhost, partition 0,ANY, 2016 bytes)
2018-04-28 20:03:42,552 - org.apache.spark.executor.Executor - INFO - Running task 0.0 in stage 10.0 (TID 14)
2018-04-28 20:03:42,555 - org.apache.spark.streaming.kafka.KafkaRDD - INFO - Beginning offset 20 is the same as ending offset skipping http_src_topic 0
2018-04-28 20:03:42,556 - org.apache.spark.executor.Executor - INFO - Finished task 0.0 in stage 10.0 (TID 14). 915 bytes result sent to driver
2018-04-28 20:03:42,557 - org.apache.spark.scheduler.TaskSetManager - INFO - Finished task 0.0 in stage 10.0 (TID 14) in 6 ms on localhost (1/1)
2018-04-28 20:03:42,558 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Removed TaskSet 10.0, whose tasks have all completed, from pool 
2018-04-28 20:03:42,559 - org.apache.spark.scheduler.DAGScheduler - INFO - ResultStage 10 (print at BluevigilConsumer.java:200) finished in 0.008 s
2018-04-28 20:03:42,560 - org.apache.spark.scheduler.DAGScheduler - INFO - Job 10 finished: print at BluevigilConsumer.java:200, took 0.029920 s
2018-04-28 20:03:42,567 - org.apache.spark.SparkContext - INFO - Starting job: print at BluevigilConsumer.java:200
2018-04-28 20:03:42,569 - org.apache.spark.scheduler.DAGScheduler - INFO - Got job 11 (print at BluevigilConsumer.java:200) with 1 output partitions
2018-04-28 20:03:42,570 - org.apache.spark.scheduler.DAGScheduler - INFO - Final stage: ResultStage 11 (print at BluevigilConsumer.java:200)
2018-04-28 20:03:42,570 - org.apache.spark.scheduler.DAGScheduler - INFO - Parents of final stage: List()
2018-04-28 20:03:42,570 - org.apache.spark.scheduler.DAGScheduler - INFO - Missing parents: List()
2018-04-28 20:03:42,572 - org.apache.spark.scheduler.DAGScheduler - INFO - Submitting ResultStage 11 (MapPartitionsRDD[11] at mapToPair at BluevigilConsumer.java:136), which has no missing parents
2018-04-28 20:03:42,575 - org.apache.spark.storage.MemoryStore - INFO - Block broadcast_11 stored as values in memory (estimated size 4.6 KB, free 124.1 KB)
2018-04-28 20:03:42,578 - org.apache.spark.storage.MemoryStore - INFO - Block broadcast_11_piece0 stored as bytes in memory (estimated size 2.6 KB, free 126.7 KB)
2018-04-28 20:03:42,579 - org.apache.spark.storage.BlockManagerInfo - INFO - Added broadcast_11_piece0 in memory on localhost:42133 (size: 2.6 KB, free: 257.8 MB)
2018-04-28 20:03:42,579 - org.apache.spark.SparkContext - INFO - Created broadcast 11 from broadcast at DAGScheduler.scala:1006
2018-04-28 20:03:42,580 - org.apache.spark.scheduler.DAGScheduler - INFO - Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[11] at mapToPair at BluevigilConsumer.java:136)
2018-04-28 20:03:42,580 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Adding task set 11.0 with 1 tasks
2018-04-28 20:03:42,582 - org.apache.spark.scheduler.TaskSetManager - INFO - Starting task 0.0 in stage 11.0 (TID 15, localhost, partition 1,ANY, 2016 bytes)
2018-04-28 20:03:42,582 - org.apache.spark.executor.Executor - INFO - Running task 0.0 in stage 11.0 (TID 15)
2018-04-28 20:03:42,587 - org.apache.spark.streaming.kafka.KafkaRDD - INFO - Computing topic http_src_topic, partition 1 offsets 21 -> 22
2018-04-28 20:03:42,588 - kafka.utils.VerifiableProperties - INFO - Verifying properties
2018-04-28 20:03:42,588 - kafka.utils.VerifiableProperties - INFO - Property group.id is overridden to 
2018-04-28 20:03:42,590 - kafka.utils.VerifiableProperties - INFO - Property zookeeper.connect is overridden to localhost:2181
2018-04-28 20:03:42,956 - org.apache.spark.storage.BlockManagerInfo - INFO - Removed broadcast_10_piece0 on localhost:42133 in memory (size: 2.6 KB, free: 257.8 MB)
2018-04-28 20:03:42,957 - org.apache.spark.ContextCleaner - INFO - Cleaned accumulator 11
2018-04-28 20:03:42,959 - org.apache.spark.storage.BlockManagerInfo - INFO - Removed broadcast_9_piece0 on localhost:42133 in memory (size: 25.5 KB, free: 257.8 MB)
2018-04-28 20:03:42,959 - org.apache.spark.ContextCleaner - INFO - Cleaned accumulator 10
2018-04-28 20:03:42,961 - org.apache.spark.storage.BlockManagerInfo - INFO - Removed broadcast_8_piece0 on localhost:42133 in memory (size: 2.6 KB, free: 257.8 MB)
2018-04-28 20:03:42,962 - org.apache.spark.ContextCleaner - INFO - Cleaned accumulator 9
2018-04-28 20:03:42,963 - org.apache.spark.storage.BlockManagerInfo - INFO - Removed broadcast_7_piece0 on localhost:42133 in memory (size: 2.6 KB, free: 257.8 MB)
2018-04-28 20:03:42,964 - org.apache.spark.ContextCleaner - INFO - Cleaned accumulator 8
2018-04-28 20:03:43,584 - org.apache.spark.executor.Executor - ERROR - Exception in task 0.0 in stage 11.0 (TID 15)
java.io.NotSerializableException: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":14,"row":"1510665301.812693|\"216.58.208.78\"|80|\"CwnBez21MgQHyUQvu8\"|","families":{"pri":[{"qualifier":"sta_msg","vlen":4,"tag":[],"timestamp":9223372036854775807},{"qualifier":"res_mim_typ","vlen":29,"tag":[],"timestamp":9223372036854775807},{"qualifier":"ver","vlen":5,"tag":[],"timestamp":9223372036854775807},{"qualifier":"id_ori_p","vlen":5,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:239)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-28 20:03:43,668 - org.apache.spark.scheduler.TaskSetManager - ERROR - Task 0.0 in stage 11.0 (TID 15) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":14,"row":"1510665301.812693|\"216.58.208.78\"|80|\"CwnBez21MgQHyUQvu8\"|","families":{"pri":[{"qualifier":"sta_msg","vlen":4,"tag":[],"timestamp":9223372036854775807},{"qualifier":"res_mim_typ","vlen":29,"tag":[],"timestamp":9223372036854775807},{"qualifier":"ver","vlen":5,"tag":[],"timestamp":9223372036854775807},{"qualifier":"id_ori_p","vlen":5,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1); not retrying
2018-04-28 20:03:43,674 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Removed TaskSet 11.0, whose tasks have all completed, from pool 
2018-04-28 20:03:43,686 - org.apache.spark.scheduler.TaskSchedulerImpl - INFO - Cancelling stage 11
2018-04-28 20:03:43,691 - org.apache.spark.scheduler.DAGScheduler - INFO - ResultStage 11 (print at BluevigilConsumer.java:200) failed in 1.108 s
2018-04-28 20:03:43,696 - org.apache.spark.scheduler.DAGScheduler - INFO - Job 11 failed: print at BluevigilConsumer.java:200, took 1.127980 s
2018-04-28 20:03:43,697 - org.apache.spark.streaming.scheduler.JobScheduler - INFO - Finished job streaming job 1524935020000 ms.1 from job set of time 1524935020000 ms
2018-04-28 20:03:43,698 - org.apache.spark.rdd.MapPartitionsRDD - INFO - Removing RDD 8 from persistence list
2018-04-28 20:03:43,698 - org.apache.spark.streaming.scheduler.JobScheduler - INFO - Total delay: 3.697 s for time 1524935020000 ms (execution: 3.672 s)
2018-04-28 20:03:43,699 - org.apache.spark.storage.BlockManager - INFO - Removing RDD 8
2018-04-28 20:03:43,700 - org.apache.spark.rdd.MapPartitionsRDD - INFO - Removing RDD 7 from persistence list
2018-04-28 20:03:43,701 - org.apache.spark.storage.BlockManager - INFO - Removing RDD 7
2018-04-28 20:03:43,701 - org.apache.spark.streaming.kafka.KafkaRDD - INFO - Removing RDD 6 from persistence list
2018-04-28 20:03:43,702 - org.apache.spark.storage.BlockManager - INFO - Removing RDD 6
2018-04-28 20:03:43,702 - org.apache.spark.streaming.scheduler.ReceivedBlockTracker - INFO - Deleting batches ArrayBuffer()
2018-04-28 20:03:43,702 - org.apache.spark.streaming.scheduler.InputInfoTracker - INFO - remove old batch metadata: 1524935000000 ms
2018-04-28 20:03:43,721 - org.apache.spark.streaming.scheduler.JobScheduler - ERROR - Error running job streaming job 1524935020000 ms.1
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0 in stage 11.0 (TID 15) had a not serializable result: org.apache.hadoop.hbase.io.ImmutableBytesWritable
Serialization stack:
	- object not serializable (class: org.apache.hadoop.hbase.io.ImmutableBytesWritable, value: )
	- field (class: scala.Tuple2, name: _1, type: class java.lang.Object)
	- object (class scala.Tuple2, (,{"totalColumns":14,"row":"1510665301.812693|\"216.58.208.78\"|80|\"CwnBez21MgQHyUQvu8\"|","families":{"pri":[{"qualifier":"sta_msg","vlen":4,"tag":[],"timestamp":9223372036854775807},{"qualifier":"res_mim_typ","vlen":29,"tag":[],"timestamp":9223372036854775807},{"qualifier":"ver","vlen":5,"tag":[],"timestamp":9223372036854775807},{"qualifier":"id_ori_p","vlen":5,"tag":[],"timestamp":9223372036854775807}]}}))
	- element of array (index: 0)
	- array (class [Ljava.lang.Object;, size 1)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1314)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1288)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$5$1.apply(DStream.scala:768)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$print$2$$anonfun$foreachFunc$5$1.apply(DStream.scala:767)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:426)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:49)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:224)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:223)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-04-28 20:03:43,883 - org.apache.spark.streaming.StreamingContext - INFO - Invoking stop(stopGracefully=false) from shutdown hook
2018-04-28 20:03:43,887 - org.apache.spark.streaming.scheduler.JobGenerator - INFO - Stopping JobGenerator immediately
2018-04-28 20:03:43,916 - org.apache.spark.streaming.util.RecurringTimer - INFO - Stopped timer for JobGenerator after time 1524935020000
2018-04-28 20:03:43,923 - org.apache.spark.streaming.scheduler.JobGenerator - INFO - Stopped JobGenerator
2018-04-28 20:03:43,935 - org.apache.spark.streaming.scheduler.JobScheduler - INFO - Stopped JobScheduler
2018-04-28 20:03:44,087 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/streaming,null}
2018-04-28 20:03:44,089 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/streaming/batch,null}
2018-04-28 20:03:44,111 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/static/streaming,null}
2018-04-28 20:03:44,113 - org.apache.spark.streaming.StreamingContext - INFO - StreamingContext stopped successfully
2018-04-28 20:03:44,122 - org.apache.spark.SparkContext - INFO - Invoking stop() from shutdown hook
2018-04-28 20:03:44,148 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
2018-04-28 20:03:44,148 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/streaming/json,null}
2018-04-28 20:03:44,149 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2018-04-28 20:03:44,150 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2018-04-28 20:03:44,150 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/api,null}
2018-04-28 20:03:44,151 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/,null}
2018-04-28 20:03:44,151 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/static,null}
2018-04-28 20:03:44,152 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2018-04-28 20:03:44,152 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2018-04-28 20:03:44,153 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2018-04-28 20:03:44,153 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors,null}
2018-04-28 20:03:44,154 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2018-04-28 20:03:44,154 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/environment,null}
2018-04-28 20:03:44,155 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2018-04-28 20:03:44,155 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2018-04-28 20:03:44,156 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2018-04-28 20:03:44,156 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage,null}
2018-04-28 20:03:44,157 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2018-04-28 20:03:44,157 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2018-04-28 20:03:44,157 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2018-04-28 20:03:44,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2018-04-28 20:03:44,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2018-04-28 20:03:44,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages,null}
2018-04-28 20:03:44,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2018-04-28 20:03:44,159 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2018-04-28 20:03:44,159 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2018-04-28 20:03:44,159 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs,null}
2018-04-28 20:03:44,214 - org.apache.spark.ui.SparkUI - INFO - Stopped Spark web UI at http://192.168.1.33:4040
2018-04-28 20:03:44,414 - org.apache.spark.MapOutputTrackerMasterEndpoint - INFO - MapOutputTrackerMasterEndpoint stopped!
2018-04-28 20:03:44,478 - org.apache.spark.storage.MemoryStore - INFO - MemoryStore cleared
2018-04-28 20:03:44,480 - org.apache.spark.storage.BlockManager - INFO - BlockManager stopped
2018-04-28 20:03:44,511 - org.apache.spark.storage.BlockManagerMaster - INFO - BlockManagerMaster stopped
2018-04-28 20:03:44,529 - org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - INFO - OutputCommitCoordinator stopped!
2018-04-28 20:03:44,574 - org.apache.spark.SparkContext - INFO - Successfully stopped SparkContext
2018-04-28 20:03:44,612 - org.apache.spark.util.ShutdownHookManager - INFO - Shutdown hook called
2018-04-28 20:03:44,614 - org.apache.spark.util.ShutdownHookManager - INFO - Deleting directory /tmp/spark-a9ea298b-eaaf-45d5-9117-4a4ca165e77f
2018-04-29 12:32:41,105 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 14:32:42,606 - org.apache.spark.SparkContext - INFO - Running Spark version 1.6.0
2018-04-29 14:32:43,255 - org.apache.hadoop.util.NativeCodeLoader - WARN - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-04-29 14:32:43,764 - org.apache.spark.util.Utils - WARN - Your hostname, AsifHost resolves to a loopback address: 127.0.1.1; using 192.168.1.33 instead (on interface wlan0)
2018-04-29 14:32:43,765 - org.apache.spark.util.Utils - WARN - Set SPARK_LOCAL_IP if you need to bind to another address
2018-04-29 14:32:43,870 - org.apache.spark.SecurityManager - INFO - Changing view acls to: asif
2018-04-29 14:32:43,871 - org.apache.spark.SecurityManager - INFO - Changing modify acls to: asif
2018-04-29 14:32:43,874 - org.apache.spark.SecurityManager - INFO - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(asif); users with modify permissions: Set(asif)
2018-04-29 14:32:44,765 - org.apache.spark.util.Utils - INFO - Successfully started service 'sparkDriver' on port 40446.
2018-04-29 14:32:45,493 - akka.event.slf4j.Slf4jLogger - INFO - Slf4jLogger started
2018-04-29 14:32:45,577 - Remoting - INFO - Starting remoting
2018-04-29 14:32:46,039 - Remoting - INFO - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.33:47082]
2018-04-29 14:32:46,072 - org.apache.spark.util.Utils - INFO - Successfully started service 'sparkDriverActorSystem' on port 47082.
2018-04-29 14:32:46,151 - org.apache.spark.SparkEnv - INFO - Registering MapOutputTracker
2018-04-29 14:32:46,244 - org.apache.spark.SparkEnv - INFO - Registering BlockManagerMaster
2018-04-29 14:32:46,272 - org.apache.spark.storage.DiskBlockManager - INFO - Created local directory at /tmp/blockmgr-e004a675-306b-442d-a037-fa7cd8f8382a
2018-04-29 14:32:46,314 - org.apache.spark.storage.MemoryStore - INFO - MemoryStore started with capacity 257.8 MB
2018-04-29 14:32:46,518 - org.apache.spark.SparkEnv - INFO - Registering OutputCommitCoordinator
2018-04-29 14:32:47,024 - org.spark-project.jetty.server.Server - INFO - jetty-8.y.z-SNAPSHOT
2018-04-29 14:32:47,114 - org.spark-project.jetty.server.AbstractConnector - INFO - Started SelectChannelConnector@0.0.0.0:4040
2018-04-29 14:32:47,115 - org.apache.spark.util.Utils - INFO - Successfully started service 'SparkUI' on port 4040.
2018-04-29 14:32:47,117 - org.apache.spark.ui.SparkUI - INFO - Started SparkUI at http://192.168.1.33:4040
2018-04-29 14:32:47,385 - org.apache.spark.executor.Executor - INFO - Starting executor ID driver on host localhost
2018-04-29 14:32:47,443 - org.apache.spark.util.Utils - INFO - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 54118.
2018-04-29 14:32:47,445 - org.apache.spark.network.netty.NettyBlockTransferService - INFO - Server created on 54118
2018-04-29 14:32:47,446 - org.apache.spark.storage.BlockManagerMaster - INFO - Trying to register BlockManager
2018-04-29 14:32:47,451 - org.apache.spark.storage.BlockManagerMasterEndpoint - INFO - Registering block manager localhost:54118 with 257.8 MB RAM, BlockManagerId(driver, localhost, 54118)
2018-04-29 14:32:47,455 - org.apache.spark.storage.BlockManagerMaster - INFO - Registered BlockManager
2018-04-29 14:32:48,734 - org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper - INFO - Process identifier=hconnection-0x645b43 connecting to ZooKeeper ensemble=localhost:2181
2018-04-29 14:32:48,757 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
2018-04-29 14:32:48,757 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:host.name=AsifHost
2018-04-29 14:32:48,758 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.version=1.8.0_161
2018-04-29 14:32:48,758 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.vendor=Oracle Corporation
2018-04-29 14:32:48,758 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.home=/opt/java/jdk1.8.0_161/jre
2018-04-29 14:32:48,758 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.class.path=/home/asif/git/bluevigil-backend-hdp/bluevigil-parent/bluevigil-streaming-core/target/classes:/home/asif/.m2/repository/javax/xml/bind/jaxb-api/2.2.7/jaxb-api-2.2.7.jar:/home/asif/.m2/repository/org/json/json/20180130/json-20180130.jar:/home/asif/.m2/repository/com/googlecode/json-simple/json-simple/1.1.1/json-simple-1.1.1.jar:/home/asif/.m2/repository/com/google/code/gson/gson/2.8.2/gson-2.8.2.jar:/home/asif/.m2/repository/org/apache/spark/spark-core_2.10/1.6.0/spark-core_2.10-1.6.0.jar:/home/asif/.m2/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/home/asif/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/home/asif/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/home/asif/.m2/repository/com/twitter/chill_2.10/0.5.0/chill_2.10-0.5.0.jar:/home/asif/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/home/asif/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/home/asif/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/home/asif/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/home/asif/.m2/repository/com/twitter/chill-java/0.5.0/chill-java-0.5.0.jar:/home/asif/.m2/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.2.0/hadoop-mapreduce-client-app-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.2.0/hadoop-mapreduce-client-common-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.2.0/hadoop-yarn-client-2.2.0.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-grizzly2/1.9/jersey-test-framework-grizzly2-1.9.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-core/1.9/jersey-test-framework-core-1.9.jar:/home/asif/.m2/repository/javax/servlet/javax.servlet-api/3.0.1/javax.servlet-api-3.0.1.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-grizzly2/1.9/jersey-grizzly2-1.9.jar:/home/asif/.m2/repository/org/glassfish/grizzly/grizzly-http/2.1.2/grizzly-http-2.1.2.jar:/home/asif/.m2/repository/org/glassfish/grizzly/grizzly-framework/2.1.2/grizzly-framework-2.1.2.jar:/home/asif/.m2/repository/org/glassfish/gmbal/gmbal-api-only/3.0.0-b023/gmbal-api-only-3.0.0-b023.jar:/home/asif/.m2/repository/org/glassfish/external/management-api/3.0.0-b012/management-api-3.0.0-b012.jar:/home/asif/.m2/repository/org/glassfish/grizzly/grizzly-http-server/2.1.2/grizzly-http-server-2.1.2.jar:/home/asif/.m2/repository/org/glassfish/grizzly/grizzly-rcm/2.1.2/grizzly-rcm-2.1.2.jar:/home/asif/.m2/repository/org/glassfish/grizzly/grizzly-http-servlet/2.1.2/grizzly-http-servlet-2.1.2.jar:/home/asif/.m2/repository/org/glassfish/javax.servlet/3.1/javax.servlet-3.1.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/home/asif/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/home/asif/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/home/asif/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/home/asif/.m2/repository/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/home/asif/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.2.0/hadoop-yarn-server-common-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.2.0/hadoop-mapreduce-client-shuffle-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.2.0/hadoop-yarn-api-2.2.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.2.0/hadoop-mapreduce-client-jobclient-2.2.0.jar:/home/asif/.m2/repository/org/apache/spark/spark-launcher_2.10/1.6.0/spark-launcher_2.10-1.6.0.jar:/home/asif/.m2/repository/org/apache/spark/spark-network-common_2.10/1.6.0/spark-network-common_2.10-1.6.0.jar:/home/asif/.m2/repository/org/apache/spark/spark-network-shuffle_2.10/1.6.0/spark-network-shuffle_2.10-1.6.0.jar:/home/asif/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/home/asif/.m2/repository/org/apache/spark/spark-unsafe_2.10/1.6.0/spark-unsafe_2.10-1.6.0.jar:/home/asif/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/home/asif/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/home/asif/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/home/asif/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/home/asif/.m2/repository/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar:/home/asif/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/home/asif/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/home/asif/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/asif/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/home/asif/.m2/repository/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar:/home/asif/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.10/jcl-over-slf4j-1.7.10.jar:/home/asif/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/home/asif/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/home/asif/.m2/repository/org/xerial/snappy/snappy-java/1.1.2/snappy-java-1.1.2.jar:/home/asif/.m2/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/home/asif/.m2/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/home/asif/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/home/asif/.m2/repository/com/typesafe/akka/akka-remote_2.10/2.3.11/akka-remote_2.10-2.3.11.jar:/home/asif/.m2/repository/com/typesafe/akka/akka-actor_2.10/2.3.11/akka-actor_2.10-2.3.11.jar:/home/asif/.m2/repository/com/typesafe/config/1.2.1/config-1.2.1.jar:/home/asif/.m2/repository/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar:/home/asif/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/home/asif/.m2/repository/com/typesafe/akka/akka-slf4j_2.10/2.3.11/akka-slf4j_2.10-2.3.11.jar:/home/asif/.m2/repository/org/scala-lang/scala-library/2.10.5/scala-library-2.10.5.jar:/home/asif/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar:/home/asif/.m2/repository/org/json4s/json4s-core_2.10/3.2.10/json4s-core_2.10-3.2.10.jar:/home/asif/.m2/repository/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10-3.2.10.jar:/home/asif/.m2/repository/org/scala-lang/scalap/2.10.0/scalap-2.10.0.jar:/home/asif/.m2/repository/org/scala-lang/scala-compiler/2.10.0/scala-compiler-2.10.0.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/home/asif/.m2/repository/asm/asm/3.1/asm-3.1.jar:/home/asif/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/home/asif/.m2/repository/org/apache/mesos/mesos/0.21.1/mesos-0.21.1-shaded-protobuf.jar:/home/asif/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar:/home/asif/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/home/asif/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/home/asif/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/home/asif/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/home/asif/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/home/asif/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/home/asif/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/home/asif/.m2/repository/org/tachyonproject/tachyon-client/0.8.2/tachyon-client-0.8.2.jar:/home/asif/.m2/repository/org/tachyonproject/tachyon-underfs-hdfs/0.8.2/tachyon-underfs-hdfs-0.8.2.jar:/home/asif/.m2/repository/org/tachyonproject/tachyon-underfs-s3/0.8.2/tachyon-underfs-s3-0.8.2.jar:/home/asif/.m2/repository/org/tachyonproject/tachyon-underfs-local/0.8.2/tachyon-underfs-local-0.8.2.jar:/home/asif/.m2/repository/net/razorvine/pyrolite/4.9/pyrolite-4.9.jar:/home/asif/.m2/repository/net/sf/py4j/py4j/0.9/py4j-0.9.jar:/home/asif/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/home/asif/.m2/repository/org/apache/spark/spark-streaming_2.10/1.6.0/spark-streaming_2.10-1.6.0.jar:/home/asif/.m2/repository/org/apache/spark/spark-streaming-kafka_2.10/1.6.0/spark-streaming-kafka_2.10-1.6.0.jar:/home/asif/.m2/repository/org/apache/kafka/kafka_2.10/0.8.2.1/kafka_2.10-0.8.2.1.jar:/home/asif/.m2/repository/org/apache/kafka/kafka-clients/0.8.2.1/kafka-clients-0.8.2.1.jar:/home/asif/.m2/repository/com/101tec/zkclient/0.3/zkclient-0.3.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-client/1.0.1/hbase-client-1.0.1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-annotations/1.0.1/hbase-annotations-1.0.1.jar:/opt/java/jdk1.8.0_161/lib/tools.jar:/home/asif/.m2/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/home/asif/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/home/asif/.m2/repository/commons-logging/commons-logging/1.2/commons-logging-1.2.jar:/home/asif/.m2/repository/com/google/guava/guava/12.0.1/guava-12.0.1.jar:/home/asif/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/home/asif/.m2/repository/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6.jar:/home/asif/.m2/repository/org/apache/htrace/htrace-core/3.1.0-incubating/htrace-core-3.1.0-incubating.jar:/home/asif/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:/home/asif/.m2/repository/org/jruby/jcodings/jcodings/1.0.8/jcodings-1.0.8.jar:/home/asif/.m2/repository/org/jruby/joni/joni/2.1.2/joni-2.1.2.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-auth/2.5.1/hadoop-auth-2.5.1.jar:/home/asif/.m2/repository/org/apache/directory/server/apacheds-kerberos-codec/2.0.0-M15/apacheds-kerberos-codec-2.0.0-M15.jar:/home/asif/.m2/repository/org/apache/directory/server/apacheds-i18n/2.0.0-M15/apacheds-i18n-2.0.0-M15.jar:/home/asif/.m2/repository/org/apache/directory/api/api-asn1-api/1.0.0-M20/api-asn1-api-1.0.0-M20.jar:/home/asif/.m2/repository/org/apache/directory/api/api-util/1.0.0-M20/api-util-1.0.0-M20.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-common/2.5.1/hadoop-common-2.5.1.jar:/home/asif/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/home/asif/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/home/asif/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/home/asif/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/home/asif/.m2/repository/com/jcraft/jsch/0.1.42/jsch-0.1.42.jar:/home/asif/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/home/asif/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.5.1/hadoop-mapreduce-client-core-2.5.1.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.5.1/hadoop-yarn-common-2.5.1.jar:/home/asif/.m2/repository/com/github/stephenc/findbugs/findbugs-annotations/1.3.9-1/findbugs-annotations-1.3.9-1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-common/1.0.1/hbase-common-1.0.1.jar:/home/asif/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/home/asif/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-server/1.0.1/hbase-server-1.0.1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-prefix-tree/1.0.1/hbase-prefix-tree-1.0.1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-common/1.0.1/hbase-common-1.0.1-tests.jar:/home/asif/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/home/asif/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/asif/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/home/asif/.m2/repository/org/apache/commons/commons-math/2.2/commons-math-2.2.jar:/home/asif/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/home/asif/.m2/repository/org/mortbay/jetty/jetty-sslengine/6.1.26/jetty-sslengine-6.1.26.jar:/home/asif/.m2/repository/org/mortbay/jetty/jsp-2.1/6.1.14/jsp-2.1-6.1.14.jar:/home/asif/.m2/repository/org/mortbay/jetty/jsp-api-2.1/6.1.14/jsp-api-2.1-6.1.14.jar:/home/asif/.m2/repository/org/mortbay/jetty/servlet-api-2.5/6.1.14/servlet-api-2.5-6.1.14.jar:/home/asif/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/home/asif/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.8/jackson-jaxrs-1.8.8.jar:/home/asif/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/home/asif/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/home/asif/.m2/repository/org/jamon/jamon-runtime/2.3.1/jamon-runtime-2.3.1.jar:/home/asif/.m2/repository/com/lmax/disruptor/3.3.0/disruptor-3.3.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.5.1/hadoop-hdfs-2.5.1.jar:/home/asif/.m2/repository/commons-daemon/commons-daemon/1.0.13/commons-daemon-1.0.13.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-hadoop-compat/1.0.1/hbase-hadoop-compat-1.0.1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-protocol/1.0.1/hbase-protocol-1.0.1.jar:/home/asif/.m2/repository/org/apache/hbase/hbase-hadoop2-compat/1.0.1/hbase-hadoop2-compat-1.0.1.jar:/home/asif/.m2/repository/org/apache/phoenix/phoenix-core/4.10.0-HBase-1.2/phoenix-core-4.10.0-HBase-1.2.jar:/home/asif/.m2/repository/org/apache/tephra/tephra-api/0.9.0-incubating/tephra-api-0.9.0-incubating.jar:/home/asif/.m2/repository/org/apache/tephra/tephra-core/0.9.0-incubating/tephra-core-0.9.0-incubating.jar:/home/asif/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/home/asif/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/home/asif/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/home/asif/.m2/repository/com/google/inject/extensions/guice-assistedinject/3.0/guice-assistedinject-3.0.jar:/home/asif/.m2/repository/org/apache/thrift/libthrift/0.9.0/libthrift-0.9.0.jar:/home/asif/.m2/repository/it/unimi/dsi/fastutil/6.5.6/fastutil-6.5.6.jar:/home/asif/.m2/repository/org/apache/twill/twill-common/0.6.0-incubating/twill-common-0.6.0-incubating.jar:/home/asif/.m2/repository/org/apache/twill/twill-core/0.6.0-incubating/twill-core-0.6.0-incubating.jar:/home/asif/.m2/repository/org/apache/twill/twill-api/0.6.0-incubating/twill-api-0.6.0-incubating.jar:/home/asif/.m2/repository/org/ow2/asm/asm-all/5.0.2/asm-all-5.0.2.jar:/home/asif/.m2/repository/org/apache/twill/twill-discovery-api/0.6.0-incubating/twill-discovery-api-0.6.0-incubating.jar:/home/asif/.m2/repository/org/apache/twill/twill-discovery-core/0.6.0-incubating/twill-discovery-core-0.6.0-incubating.jar:/home/asif/.m2/repository/org/apache/twill/twill-zookeeper/0.6.0-incubating/twill-zookeeper-0.6.0-incubating.jar:/home/asif/.m2/repository/org/apache/tephra/tephra-hbase-compat-1.1/0.9.0-incubating/tephra-hbase-compat-1.1-0.9.0-incubating.jar:/home/asif/.m2/repository/org/antlr/antlr-runtime/3.5.2/antlr-runtime-3.5.2.jar:/home/asif/.m2/repository/jline/jline/2.11/jline-2.11.jar:/home/asif/.m2/repository/sqlline/sqlline/1.2.0/sqlline-1.2.0.jar:/home/asif/.m2/repository/joda-time/joda-time/1.6/joda-time-1.6.jar:/home/asif/.m2/repository/com/github/stephenc/jcip/jcip-annotations/1.0-1/jcip-annotations-1.0-1.jar:/home/asif/.m2/repository/org/apache/httpcomponents/httpclient/4.0.1/httpclient-4.0.1.jar:/home/asif/.m2/repository/org/apache/httpcomponents/httpcore/4.0.1/httpcore-4.0.1.jar:/home/asif/.m2/repository/org/iq80/snappy/snappy/0.3/snappy-0.3.jar:/home/asif/.m2/repository/org/apache/commons/commons-csv/1.0/commons-csv-1.0.jar:/home/asif/.m2/repository/org/apache/hadoop/hadoop-annotations/2.7.1/hadoop-annotations-2.7.1.jar:/home/asif/.m2/repository/org/apache/phoenix/phoenix-queryserver-client/4.10.0-HBase-1.2/phoenix-queryserver-client-4.10.0-HBase-1.2.jar:/home/asif/.m2/repository/org/apache/calcite/avatica/avatica-core/1.9.0/avatica-core-1.9.0.jar:/home/asif/.m2/repository/org/apache/calcite/avatica/avatica-metrics/1.9.0/avatica-metrics-1.9.0.jar:/home/asif/.m2/repository/com/maxmind/geoip2/geoip2/2.10.0/geoip2-2.10.0.jar:/home/asif/.m2/repository/com/maxmind/db/maxmind-db/1.2.2/maxmind-db-1.2.2.jar:/home/asif/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.7.4/jackson-databind-2.7.4.jar:/home/asif/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar:/home/asif/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.7.4/jackson-core-2.7.4.jar:/home/asif/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.10/2.7.4/jackson-module-scala_2.10-2.7.4.jar:/home/asif/.m2/repository/org/scala-lang/scala-reflect/2.10.6/scala-reflect-2.10.6.jar:/home/asif/.m2/repository/com/fasterxml/jackson/module/jackson-module-paranamer/2.7.4/jackson-module-paranamer-2.7.4.jar:/home/asif/.m2/repository/com/thoughtworks/paranamer/paranamer/2.8/paranamer-2.8.jar:/home/asif/.m2/repository/org/apache/pig/pig/0.17.0/pig-0.17.0.jar:/home/asif/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/home/asif/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/home/asif/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/home/asif/.m2/repository/hsqldb/hsqldb/1.8.0.10/hsqldb-1.8.0.10.jar:/home/asif/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/home/asif/.m2/repository/dk/brics/automaton/automaton/1.11-8/automaton-1.11-8.jar:/home/asif/.m2/repository/org/apache/avro/avro/1.7.5/avro-1.7.5.jar:/home/asif/.m2/repository/org/codehaus/groovy/groovy-all/2.4.5/groovy-all-2.4.5.jar:/home/asif/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/asif/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar
2018-04-29 14:32:48,760 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.library.path=/opt/java/jdk1.8.0_161/jre/lib/i386/server:/opt/java/jdk1.8.0_161/jre/lib/i386:/opt/java/jdk1.8.0_161/jre/../lib/i386:/opt/java/jdk1.8.0_161/jre/lib/i386/client:/opt/java/jdk1.8.0_161/jre/lib/i386::/usr/java/packages/lib/i386:/lib:/usr/lib
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.io.tmpdir=/tmp
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:java.compiler=<NA>
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:os.name=Linux
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:os.arch=i386
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:os.version=3.13.0-37-generic
2018-04-29 14:32:48,761 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:user.name=asif
2018-04-29 14:32:48,762 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:user.home=/home/asif
2018-04-29 14:32:48,762 - org.apache.zookeeper.ZooKeeper - INFO - Client environment:user.dir=/home/asif/git/bluevigil-backend-hdp/bluevigil-parent/bluevigil-streaming-core
2018-04-29 14:32:48,763 - org.apache.zookeeper.ZooKeeper - INFO - Initiating client connection, connectString=localhost:2181 sessionTimeout=90000 watcher=hconnection-0x645b430x0, quorum=localhost:2181, baseZNode=/hbase
2018-04-29 14:32:48,792 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:48,807 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:48,959 - org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper - WARN - Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
2018-04-29 14:32:48,960 - org.apache.hadoop.hbase.util.RetryCounter - INFO - Sleeping 1000ms before retry #0...
2018-04-29 14:32:49,942 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:49,944 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:50,045 - org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper - WARN - Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
2018-04-29 14:32:50,045 - org.apache.hadoop.hbase.util.RetryCounter - INFO - Sleeping 2000ms before retry #1...
2018-04-29 14:32:51,045 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:51,046 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:52,147 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:52,148 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:52,249 - org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper - WARN - Possibly transient ZooKeeper, quorum=localhost:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/hbaseid
2018-04-29 14:32:52,249 - org.apache.hadoop.hbase.util.RetryCounter - INFO - Sleeping 4000ms before retry #2...
2018-04-29 14:32:53,249 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:53,250 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:54,351 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:54,352 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:55,453 - org.apache.zookeeper.ClientCnxn - INFO - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-04-29 14:32:55,454 - org.apache.zookeeper.ClientCnxn - WARN - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1081)
2018-04-29 14:32:56,127 - org.apache.spark.SparkContext - INFO - Invoking stop() from shutdown hook
2018-04-29 14:32:56,145 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
2018-04-29 14:32:56,146 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
2018-04-29 14:32:56,148 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/api,null}
2018-04-29 14:32:56,149 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/,null}
2018-04-29 14:32:56,150 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/static,null}
2018-04-29 14:32:56,151 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
2018-04-29 14:32:56,152 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
2018-04-29 14:32:56,153 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors/json,null}
2018-04-29 14:32:56,155 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/executors,null}
2018-04-29 14:32:56,156 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/environment/json,null}
2018-04-29 14:32:56,156 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/environment,null}
2018-04-29 14:32:56,157 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
2018-04-29 14:32:56,157 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
2018-04-29 14:32:56,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage/json,null}
2018-04-29 14:32:56,158 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/storage,null}
2018-04-29 14:32:56,159 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
2018-04-29 14:32:56,160 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
2018-04-29 14:32:56,160 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
2018-04-29 14:32:56,160 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
2018-04-29 14:32:56,161 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages/json,null}
2018-04-29 14:32:56,162 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/stages,null}
2018-04-29 14:32:56,162 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
2018-04-29 14:32:56,163 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
2018-04-29 14:32:56,163 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
2018-04-29 14:32:56,164 - org.spark-project.jetty.server.handler.ContextHandler - INFO - stopped o.s.j.s.ServletContextHandler{/jobs,null}
2018-04-29 14:32:56,218 - org.apache.spark.ui.SparkUI - INFO - Stopped Spark web UI at http://192.168.1.33:4040
2018-04-29 14:32:56,274 - org.apache.spark.MapOutputTrackerMasterEndpoint - INFO - MapOutputTrackerMasterEndpoint stopped!
2018-04-29 14:32:56,291 - org.apache.spark.storage.MemoryStore - INFO - MemoryStore cleared
2018-04-29 14:32:56,293 - org.apache.spark.storage.BlockManager - INFO - BlockManager stopped
2018-04-29 14:32:56,311 - org.apache.spark.storage.BlockManagerMaster - INFO - BlockManagerMaster stopped
2018-04-29 14:32:56,319 - org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - INFO - OutputCommitCoordinator stopped!
2018-04-29 14:32:56,325 - org.apache.spark.SparkContext - INFO - Successfully stopped SparkContext
2018-04-29 14:32:56,326 - org.apache.spark.util.ShutdownHookManager - INFO - Shutdown hook called
2018-04-29 14:32:56,327 - org.apache.spark.util.ShutdownHookManager - INFO - Deleting directory /tmp/spark-f63bf3d6-5e1e-4d38-a0d0-64a2832a072b
2018-04-29 14:33:03,458 - org.apache.spark.SparkContext - INFO - Running Spark version 1.6.0
2018-04-29 14:33:03,778 - org.apache.hadoop.util.NativeCodeLoader - WARN - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-04-29 14:33:04,104 - org.apache.spark.util.Utils - WARN - Your hostname, AsifHost resolves to a loopback address: 127.0.1.1; using 192.168.1.33 instead (on interface wlan0)
2018-04-29 14:33:04,105 - org.apache.spark.util.Utils - WARN - Set SPARK_LOCAL_IP if you need to bind to another address
2018-04-29 14:33:04,134 - org.apache.spark.SecurityManager - INFO - Changing view acls to: asif
2018-04-29 14:33:04,135 - org.apache.spark.SecurityManager - INFO - Changing modify acls to: asif
2018-04-29 14:33:04,135 - org.apache.spark.SecurityManager - INFO - SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(asif); users with modify permissions: Set(asif)
2018-04-29 14:33:04,753 - org.apache.spark.util.Utils - INFO - Successfully started service 'sparkDriver' on port 38102.
2018-04-29 14:33:05,214 - akka.event.slf4j.Slf4jLogger - INFO - Slf4jLogger started
2018-04-29 14:33:05,314 - Remoting - INFO - Starting remoting
2018-04-29 14:33:05,603 - Remoting - INFO - Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@192.168.1.33:44810]
2018-04-29 14:33:05,613 - org.apache.spark.util.Utils - INFO - Successfully started service 'sparkDriverActorSystem' on port 44810.
2018-04-29 14:33:05,640 - org.apache.spark.SparkEnv - INFO - Registering MapOutputTracker
2018-04-29 14:33:05,677 - org.apache.spark.SparkEnv - INFO - Registering BlockManagerMaster
2018-04-29 14:33:05,699 - org.apache.spark.storage.DiskBlockManager - INFO - Created local directory at /tmp/blockmgr-45ebbae6-5302-40f7-9e5a-7810101222e0
2018-04-29 14:33:05,716 - org.apache.spark.storage.MemoryStore - INFO - MemoryStore started with capacity 257.8 MB
2018-04-29 14:33:05,839 - org.apache.spark.SparkEnv - INFO - Registering OutputCommitCoordinator
2018-04-29 14:33:06,096 - org.spark-project.jetty.server.Server - INFO - jetty-8.y.z-SNAPSHOT
2018-04-29 14:33:06,148 - org.spark-project.jetty.server.AbstractConnector - INFO - Started SelectChannelConnector@0.0.0.0:4040
2018-04-29 14:33:06,149 - org.apache.spark.util.Utils - INFO - Successfully started service 'SparkUI' on port 4040.
2018-04-29 14:33:06,152 - org.apache.spark.ui.SparkUI - INFO - Started SparkUI at http://192.168.1.33:4040
2018-04-29 14:33:06,300 - org.apache.spark.executor.Executor - INFO - Starting executor ID driver on host localhost
2018-04-29 14:33:06,329 - org.apache.spark.util.Utils - INFO - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 59551.
2018-04-29 14:33:06,330 - org.apache.spark.network.netty.NettyBlockTransferService - INFO - Server created on 59551
2018-04-29 14:33:06,332 - org.apache.spark.storage.BlockManagerMaster - INFO - Trying to register BlockManager
2018-04-29 14:33:06,336 - org.apache.spark.storage.BlockManagerMasterEndpoint - INFO - Registering block manager localhost:59551 with 257.8 MB RAM, BlockManagerId(driver, localhost, 59551)
2018-04-29 14:33:06,340 - org.apache.spark.storage.BlockManagerMaster - INFO - Registered BlockManager
2018-04-29 14:33:52,445 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 14:35:37,440 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 14:46:03,626 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 15:07:21,097 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 16:01:11,074 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-04-29 16:12:12,110 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 12:21:45,477 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 12:28:46,764 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 12:29:17,534 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 12:49:36,725 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 12:49:47,443 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-01 20:56:09,865 - org.apache.spark.SparkContext - INFO - Running Spark version 1.6.0
2018-05-01 20:56:10,577 - org.apache.hadoop.util.NativeCodeLoader - WARN - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-05-01 20:56:11,174 - org.apache.spark.SparkContext - ERROR - Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:401)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59)
	at com.bluevigil.streaming.BluevigilStreamingProcessor.main(BluevigilStreamingProcessor.java:60)
2018-05-01 20:56:11,203 - org.apache.spark.SparkContext - INFO - Successfully stopped SparkContext
2018-05-01 21:04:53,898 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 20:17:55,656 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 21:02:33,736 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 21:35:00,226 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 21:42:02,015 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 22:12:56,454 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 22:37:52,191 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 22:45:08,638 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-02 22:45:42,272 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
2018-05-06 11:30:22,691 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser null
2018-05-06 11:35:17,856 - com.bluevigil.utils.DynamicJsonParser - INFO - Dynamic json parser {"ts":1510665261.593545,"uid":"CLPj1c2y2azVqpC1Tg","id.orig_h":"192.168.100.9","id.orig_p":45224,"id.resp_h":"216.58.208.78","id.resp_p":80,"trans_depth":1,"method":"POST","host":"clients1.google.com","uri":"/ocsp","version":"1.1","user_agent":"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:56.0) Gecko/20100101 Firefox/56.0","request_body_len":75,"response_body_len":463,"status_code":200,"status_msg":"OK","tags":[],"orig_fuids":["FlD9j3hVHiVU4T0T9"],"orig_mime_types":["application/ocsp-request"],"resp_fuids":["FxXGfP2orWzwwF3RB8"],"resp_mime_types":["application/ocsp-response"]}
